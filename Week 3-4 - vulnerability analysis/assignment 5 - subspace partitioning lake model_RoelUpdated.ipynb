{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lake model continued\n",
    "\n",
    "In the previous week you used the lake problem as a means of getting aquinted with the workbench. In this assignment we will continue with the lake problem, focussing explicitly on using it for open exploration. You can use the second part of [this tutorial](https://emaworkbench.readthedocs.io/en/latest/indepth_tutorial/open-exploration.html) for help.\n",
    "\n",
    "**It is paramount that you are using the lake problem with 100 decision variables, rather than the one found on the website with the seperate anthropogenic release decision**\n",
    "\n",
    "## Apply scenario discovery\n",
    "\n",
    "1. Generate 10 policies and 1000 scenarios and evaluate them.\n",
    "2. The experiments array contains the values for each of the 100 decision levers. This might easily mess up the analysis. Remove these columns from the experiment array. *hint: use `experiments.drop`*\n",
    "3. Apply scenario discovery, focussing on the 10 percent of worst outcomes for reliability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lakemodel_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-90cf96676c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing important Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlakemodel_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlake_problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m from ema_workbench import (RealParameter, ScalarOutcome, Constant,\n\u001b[0;32m      4\u001b[0m                            Model)\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lakemodel_function'"
     ]
    }
   ],
   "source": [
    "#Importing important Libraries\n",
    "from lakemodel_function import lake_problem\n",
    "from ema_workbench import (RealParameter, ScalarOutcome, Constant,\n",
    "                           Model)\n",
    "import pandas as pd\n",
    "from ema_workbench.analysis import prim\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: This step involves the specification of Uncertainties, Levers and outcomes for the lake problem. These will be used for the open exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakemodel_function import lake_problem\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome)\n",
    "\n",
    "#instantiate the model\n",
    "lake_model = Model('lakeproblem', function=lake_problem)\n",
    "lake_model.time_horizon = 100 # used to specify the number of timesteps\n",
    "\n",
    "#specify uncertainties\n",
    "lake_model.uncertainties = [RealParameter('mean', 0.01, 0.05),\n",
    "                            RealParameter('stdev', 0.001, 0.005),\n",
    "                            RealParameter('b', 0.1, 0.45),\n",
    "                            RealParameter('q', 2.0, 4.5),\n",
    "                            RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers, one for each time step\n",
    "lake_model.levers = [RealParameter(f\"l{i}\", 0, 0.1) for i in \n",
    "                     range(lake_model.time_horizon)] # we use time_horizon here\n",
    "\n",
    "#specify outcomes \n",
    "lake_model.outcomes = [ScalarOutcome('max_P'),\n",
    "                       ScalarOutcome('utility'),\n",
    "                       ScalarOutcome('inertia'),\n",
    "                       ScalarOutcome('reliability')] #  ScalarOutcome.MINIMIZE???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: At this level, we perform the experiments for 1000 scenarios and 10 policies as specified in the problem description. For this, we use a workbench library called MultiprocessingEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import MultiprocessingEvaluator\n",
    "\n",
    "n_scenarios = 1000   #Scenario specification\n",
    "n_policies = 10      #Policiy specification\n",
    "\n",
    "#performing the experiments for given number of scenarios and policies\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    experiments, outcomes = evaluator.perform_experiments(n_scenarios, n_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the experiment results\n",
    "experiments   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: As required in the problem descripiton, we drop the values of the 100 decision levers so that they do not mess up the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []               #Generate empty list \n",
    "for x in range (100):\n",
    "    lever = \"l\"+str(x)\n",
    "    droplist.append(\"l\"+str(x))      # Append decision lever values as strings to the list\n",
    "\n",
    "print(droplist)          #print list of decision levers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_experiments= experiments.drop(droplist, axis=1)      #drop the generated list pf decision levers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_experiments   # Print new experiment results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes['reliability']   #print reliability column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.DataFrame.from_dict(outcomes)   # convert outcomes to dataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes   #print outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 4**: Here, we need to focus on the 10% worst outcomes for reliability. For this, we first of all subset the subset those values using a nsmallest function, specifying 1000 for the percentage, since 10% of 10000 values is 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_reliability = outcomes.nsmallest(1000, \"reliability\")   #subset 10% worst reliability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_reliability_df = lowest_reliability[\"reliability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_reliability_df.max()   # Find the maximum of the worst values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = adjusted_experiments\n",
    "y = outcomes['reliability'] <lowest_reliability_df.max()\n",
    "prim_alg = prim.Prim(x, y, threshold=0.1, peel_alpha = 0.11)\n",
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.show_tradeoff()            \n",
    "plt.show()         #print the results on a box plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.inspect_tradeoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the results in table and visual format\n",
    "box1.inspect(4)\n",
    "box1.inspect(4, style='graph')\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results as scatter plots \n",
    "box1.select(23) # make boxes to cover the 23 worst values \n",
    "fig = box1.show_pairs_scatter()\n",
    "fig.set_size_inches((12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results using Dimensional Stacking\n",
    "Take the classification of outcomes as used in step 3 of scenario discovery, and instead visualize the results using dimensional stacking. How do these results compare to the insights from scenario discovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import dimensional_stacking   # import the required library for dimensional stacking \n",
    "\n",
    "x = experiments\n",
    "y = outcomes['reliability'] <lowest_reliability_df.max()\n",
    "dimensional_stacking.create_pivot_plot(x,y, 2, nbins=3)\n",
    "plt.show()      #print table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
